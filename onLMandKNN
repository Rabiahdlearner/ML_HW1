```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Question 1

Euclidean distances between each observation and testpoint
```{r}
sqrt((0-0)^2 + (3-0)^2 + (0-0)^2)
sqrt((2-0)^2 + (0-0)^2 + (0-0)^2)
sqrt((0-0)^2 + (1-0)^2 + (3-0)^2)
sqrt((0-0)^2 + (1-0)^2 + (2-0)^2)
sqrt((-1-0)^2 + (0-0)^2 + (1-0)^2)
sqrt((1-0)^2 + (1-0)^2 + (1-0)^2)

```





##Question 2
This should use the Carseats dataset
###2a. Multiple regression model to predct sales using price, urban and US.
sales = B0 + 
```{r}
#Installing required package for the dataset
#install.packages("ISLR", dependencies=T)
library(ISLR)
View(Carseats)

#examine the stucture of the dataset to see if the columns have been correctly labelled
str(Carseats)
#library(dplyr)
# carseat <- Carseats |>
#   mutate(US = as.factor(as.numeric(US)),
#          Urban = as.factor(as.numeric(Urban)))
#fit of multiple regression model
fit <- lm(Sales ~ Price + Urban + US, data = Carseats)

# Check the summarised result
summary(fit)


```
###2b. Interpreting the coefficients of the model
The result of the multiple regression model fitted above shows the estimates of coefficients of the predictors; price,urban and US as -0.054, -0.022 and 1.200 respectively. each of these coefficients make its corresponding effect (predictor) cause a change in sales, when all others remain constant. 

In other words, the coefficient estimate of price of -0.054 implies a  decrease in price of car seat will result in a significant increase in sales (this significant is evident by the pvalue i.e the probability of prediction error bein close to 0;<2e-16) when other factors remain constant, i.e the sales of car seat is predicted to increase if the price goes down when other factors are constant.

Similarly, stores in urban centers (coefficient estimate Urban if yes:1 = -0.022 No:0 as seen in contrast chunk below) will have a negative effect on sales but this effect is not significant (probabilty of error being close to 1:0.936), i.e with increasing stores in urban centers the sales of carseat is predicted to be lower but this will not be significant and vice versa.

However, stores in the United States (US if yes:1 = 1.200 No:0 as seen in contrast chunk below) are predicted to have a significantly direct proportional effect of sale when other factors are kept constant, i.e carseat stores in the  United states will experience significant 1.2times higher sales (pvalue: probability of error is close to 0;4.86e-06)

```{r}
attach(Carseats)
cbind(contrasts(Urban), contrasts(US))
```

It is important to note that the intercept estimate explains the effect on sales when all other predictors (explanatory variables) are not significant.

###2c. Model Equation

###2d. Hypothesis test of significance
H0: Bj = 0 ; The effect of predictor is significant on sales of carseat
H1: Bj != 0: The effect of predictor is not significant on sales of car seat.

j = Price, Urban, US


for  price, Bj != 0 in that the pvalue = <2e-16 i.e the probability of commiting a type 1 error (rejecting H0 when it is true) is very close to 0.

Similarly, for US, Bj !=0 in that the pvalue = 4.86e-06, i.e the probability of rejecting H0 when true is close to 0.

Therefore, price and store being in US has significant effect on sales of carseats.

###2e Since the predictors with evidence of significant association with sales (outcome) are only price and US, therefore our new model will be:
```{r}
fit1 <- lm(Sales ~ Price + US, data = Carseats)
summary(fit1)

```
###2f. Assessing how well the two models we have fitted so far fits the data we can examine the p.value of both models either directly or using the analysis of variance (ANOVA)
```{r}
fit_test <- anova(fit, fit1)
fit_test
```
with the first model that included Urban,  p-value: < 2.2e-16, probability of accepting its goodness for prediction is very close to 0. which implies the model is significant and good for our prediction. Also, when we remove the non significant predictor, Urban, (fit1) the model's   p-value: < 2.2e-16, also very close to 0. This signifies the goodness of fit in the affirmative.

We went further to conduct the analysis of variance so we will be able to determine the better one and find out that the p-value is 0.936, which implies no significant difference between both models and so we can decide to use any of the models. in other words fit (model a) and fit1 (model e) fit the data equally well statistically.


###2g. Obtaining the 95% confidence interval of the coefficient estimates using the confint() in R. this is similar to the manual calculation:
  Bj ± t1-α/2, n-2 * se(Bj)
```{r}
confint(fit1, level=0.95)
```
The above shows that the 95% confidence interval of coefficient estimates of Price and US are (-0.065, -0.044) and (0.692, 1.708) respectively. The absence of 0 in these intervals indicate that these predictors truly contribute to the sales of car seats.

###2h. Evidence of outliers (determined  by examining the stadardized residual: residual divided by estimated standard error, interpreted as the number of standard error away from the residual line ) and high leverage point (determined hat value above 2(p + 1)/n: p = number of predictors; n = number of observations) can be ascertained by inspecting the residual vs leverage plot, i.e the fifth of the model's plot when using the plot function (Kassambara for STHDA, 2018). 


```{r}
#Visualizing the leverage and outliers
plot(fit1, 5)

```
The plot above show leverage point is above 0.04. Also there are observations that appears to have standardized residual around acceptable  standardized residual of between -3 and 3. 

using the studres() of the MASS package, which directly output the studentized residuals of the model residuals, then we visualize which one is beyond the acceptable range (-3, 3)
```{r}
library(MASS)
which(studres(fit1) > 3)
range(studres(fit1))

```
the above evidence shows that none of the residuals of response goes beyond the acceptable limit. it is therefore save to say that there is no significant outlier because 


Cheching for high leverage involves observing which value is above, 0.04 based on the estimation of our data's leverage point formular indicated above. this is same has hatvalues() of stats package in r
```{r}
any(hatvalues(fit1)>0.04)
which(hatvalues(fit1)>0.04)
```

the result in the chunck above shows truly, there is an oservation with high leverage and it is observation 43.

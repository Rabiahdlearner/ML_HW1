STAT 860 - Homework 1
Rabiat Atanda
2024-09-15
##Question 1
a.
Euclidean distances between each observation and testpoint sqrt((0-0)^2 + (3-0)^2 + (0-0)^2) ## [1] 3 sqrt((2-0)^2 + (0-0)^2 + (0-0)^2) ## [1] 2 sqrt((0-0)^2 + (1-0)^2 + (3-0)^2) ## [1] 3.162278 sqrt((0-0)^2 + (1-0)^2 + (2-0)^2) ## [1] 2.236068 sqrt((-1-0)^2 + (0-0)^2 + (1-0)^2) ## [1] 1.414214 sqrt((1-0)^2 + (1-0)^2 + (1-0)^2) ## [1] 1.732051
b.
the prediction of x1=x2=x3=0 is given as follows
let red = 0 and green = 1
let ùë¶ÃÇ=1ùëò Œ£ùë¶I
therefore ùë¶ÃÇ=0+0+ 03
ùë¶ÃÇ=0 red
Where ùë¶ÃÇ= predicted value of the outcome
K = number of neighbors
Yi = Actual values of the outcome with respect to the neighbors number chosen
Also, if we follow the Euclidean distance, since two out of three nearest neighbours to the Euclidean distance are 0, i.e red, then the predicted outcome will be red
##Question 2 This should use the Carseats dataset ###2a. Multiple regression model to predct sales using price, urban and US. sales = B0 + #Installing required package for the dataset #install.packages("ISLR", dependencies=T) library(ISLR) ## Warning: package 'ISLR' was built under R version 4.4.1 View(Carseats) #examine the stucture of the dataset to see if the columns have been correctly labelled str(Carseats) ## 'data.frame': 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels "Bad","Good","Medium": 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels "No","Yes": 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels "No","Yes": 2 2 2 2 1 2 1 2 1 2 ... #library(dplyr) # carseat <- Carseats |> # mutate(US = as.factor(as.numeric(US)), # Urban = as.factor(as.numeric(Urban))) #fit of multiple regression model fit <- lm(Sales ~ Price + Urban + US, data = Carseats) # Check the summarised result summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients:
## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 13.043469 0.651012 20.036 < 2e-16 *** ## Price -0.054459 0.005242 -10.389 < 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: < 2.2e-16
###2b. Interpreting the coefficients of the model The result of the multiple regression model fitted above shows the estimates of coefficients of the predictors; price,urban and US as -0.054, -0.022 and 1.200 respectively. each of these coefficients make its corresponding effect (predictor) cause a change in sales, when all others remain constant.
In other words, the coefficient estimate of price of -0.054 implies a decrease in price of car seat will result in a significant increase in sales (this significant is evident by the pvalue i.e the probability of prediction error bein close to 0;<2e-16) when other factors remain constant, i.e the sales of car seat is predicted to increase if the price goes down when other factors are constant.
Similarly, stores in urban centers (coefficient estimate Urban if yes:1 = -0.022 No:0 as seen in contrast chunk below) will have a negative effect on sales but this effect is not significant (probabilty of error being close to 1:0.936), i.e with increasing stores in urban centers the sales of carseat is predicted to be lower but this will not be significant and vice versa.
However, stores in the United States (US if yes:1 = 1.200 No:0 as seen in contrast chunk below) are predicted to have a significantly direct proportional effect of sale when other factors are kept constant, i.e carseat stores in the United states will experience significant 1.2times higher sales (pvalue: probability of error is close to 0;4.86e-06) attach(Carseats) cbind(contrasts(Urban), contrasts(US)) ## Yes Yes ## No 0 0 ## Yes 1 1
It is important to note that the intercept estimate explains the effect on sales when all other predictors (explanatory variables) are not significant.
###2c. Model Equation
###2d. Hypothesis test of significance H0: Bj = 0 ; The effect of predictor is significant on sales of carseat H1: Bj != 0: The effect of predictor is not significant on sales of car seat.
j = Price, Urban, US
for price, Bj != 0 in that the pvalue = <2e-16 i.e the probability of commiting a type 1 error (rejecting H0 when it is true) is very close to 0.
Similarly, for US, Bj !=0 in that the pvalue = 4.86e-06, i.e the probability of rejecting H0 when true is close to 0.
Therefore, price and store being in US has significant effect on sales of carseats.
###2e Since the predictors with evidence of significant association with sales (outcome) are only price and US, therefore our new model will be: fit1 <- lm(Sales ~ Price + US, data = Carseats) summary(fit1) ## ## Call: ## lm(formula = Sales ~ Price + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9269 -1.6286 -0.0574 1.5766 7.0515 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 13.03079 0.63098 20.652 < 2e-16 *** ## Price -0.05448 0.00523 -10.416 < 2e-16 *** ## USYes 1.19964 0.25846 4.641 4.71e-06 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.469 on 397 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 ## F-statistic: 62.43 on 2 and 397 DF, p-value: < 2.2e-16
###2f. Assessing how well the two models we have fitted so far fits the data we can examine the p.value of both models either directly or using the analysis of variance (ANOVA) fit_test <- anova(fit, fit1) fit_test ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(>F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357
with the first model that included Urban, p-value: < 2.2e-16, probability of accepting its goodness for prediction is very close to 0. which implies the model is significant and good
for our prediction. Also, when we remove the non significant predictor, Urban, (fit1) the model‚Äôs p-value: < 2.2e-16, also very close to 0. This signifies the goodness of fit in the affirmative for both.
However observing a lower rse and higher rsquared in the latter, we will want to believe the latter is a better model than the former.
We went further to conduct the analysis of variance so we will be able to determine the better one and find out that the p-value is 0.936, which implies no significant difference between both models and so we can decide to use any of the models. In other words fit (model a) and fit1 (model e) fit the data equally well statistically.
###2g. Obtaining the 95% confidence interval of the coefficient estimates using the confint() in R. this is similar to the manual calculation: Bj ¬± t1-Œ±/2, n-2 * se(Bj) confint(fit1, level=0.95) ## 2.5 % 97.5 % ## (Intercept) 11.79032020 14.27126531 ## Price -0.06475984 -0.04419543 ## USYes 0.69151957 1.70776632
The above shows that the 95% confidence interval of coefficient estimates of Price and US are (-0.065, -0.044) and (0.692, 1.708) respectively. The absence of 0 in these intervals indicate that these predictors truly contribute to the sales of car seats.
###2h. Evidence of outliers (determined by examining the stadardized residual: residual divided by estimated standard error, interpreted as the number of standard error away from the residual line ) and high leverage point (determined hat value above 2(p + 1)/n: p = number of predictors; n = number of observations) can be ascertained by inspecting the residual vs leverage plot, i.e the fifth of the model‚Äôs plot when using the plot function (Kassambara for STHDA, 2018). #Visualizing the leverage and outliers plot(fit1, 5)
The plot above show leverage point is above 0.04. Also there are observations that appears to have standardized residual around acceptable standardized residual of between -3 and 3.
using the studres() of the MASS package, which directly output the studentized residuals of the model residuals, then we visualize which one is beyond the acceptable range (-3, 3) library(MASS) which(studres(fit1) > 3) ## named integer(0) range(studres(fit1)) ## [1] -2.835843 2.891521
the above evidence shows that none of the residuals of response goes beyond the acceptable limit. it is therefore save to say that there is no significant outlier because
Cheching for high leverage involves observing which value is above, 0.04 based on the estimation of our data‚Äôs leverage point formular indicated above. this is same has hatvalues() of stats package in r any(hatvalues(fit1)>0.04) ## [1] TRUE which(hatvalues(fit1)>0.04)
## 43 ## 43
the result in the chunck above shows truly, there is an oservation with high leverage and it is observation 43.
